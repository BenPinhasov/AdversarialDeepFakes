# Adversarial DeepFakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples

## Overview
Deepfakes or facially manipulated videos, can be used maliciously to spread disinformation, harass individuals or defame famous personalities. Recently developed Deepfake detection methods rely on Convolutional Neural Network (CNN) based classifiers to distinguish AI-generated fake videos from real videos. In this work, we demonstrate that it is possible to bypass such detectors by adversarially modifying fake videos synthesized using existing Deepfake generation methods. We design adversarial examples for the FaceForensics++ dataset to fool Deepfake detectors. We build on top of the [FaceForensics++ codebase](https://github.com/ondyari/FaceForensics) for Deepfake detection and fool the victim DeepFake detectors.


## FaceForensics++ Dataset Access 
If you would like to download the FaceForensics++ dataset, please fill out [this google form](https://docs.google.com/forms/d/e/1FAIpQLSdRRR3L5zAv6tQ_CKxmK4W96tAab_pfBu2EKAgQbeDVhmXagg/viewform) and, once accepted, we will send you the link to our download script.

If you have not received a response within a week, it is likely that your email is bouncing - please check this before sending repeat requests.

Once, you obtain the download link, please head to the [download section](dataset/README.md). You can also find details about the generation of the dataset there.
